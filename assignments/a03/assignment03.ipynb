{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"text-align: center;\" >\n",
    "<h1 style=\"margin-top: 0.2em; margin-bottom: 0.1em;\">Assignment 3</h1>\n",
    "<h4 style=\"margin-top: 0.7em; margin-bottom: 0.3em; font-style:italic\">Commit your solutions to GitHub until June 21, 23:59</h4>\n",
    "</div>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1 \n",
    "## Sentiment Evaluation of Twitter and YouTube Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tasks\n",
    "\n",
    "1. Install packages and load evaluation datasets with Google NLP scores\n",
    "2. Run VADER over evaluation texts\n",
    "3. Run BERT over evaluation texts\n",
    "4. Evaluate against sentiment annotations and compare with Google NLP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Install requirements. \n",
    "\n",
    "The following cell contains all the necessary dependencies needed for this task. If you run the cell everything will be installed. \n",
    "\n",
    "* [`vaderSentiment`](https://github.com/cjhutto/vaderSentiment) is a Python package for a Parsimonious Rule-based Model for Sentiment Analysis of Social Media Text.\n",
    "* [`transformers`](https://huggingface.co/) is a Python package for creating and working with transformers. [Here](https://huggingface.co/docs) is the documentation of `transformers`.\n",
    "* [`torch`](https://pytorch.org/) is a Python machine learning framework. We need this here for `transformers` since this package uses internally `torch`. [Here](https://pytorch.org/docs/stable/index.html) is the documentation of `torch`.\n",
    "* [`pandas`](https://pandas.pydata.org/docs/index.html) is a Python package for creating and working with tabular data. [Here](https://pandas.pydata.org/docs/reference/index.html) is the documentation of `pandas`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "! pip install vaderSentiment\n",
    "! pip install transformers\n",
    "! pip install torch\n",
    "! pip install pandas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You may need to restart the Kernel after installing the dependencies!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import requirements\n",
    "The cell below imports all necessary dependancies. Make sure they are installed (see cell above)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "from transformers import pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 1: Load evaluation datasets and Google NLP scores\n",
    "\n",
    "#### 1.1 Load datasets\n",
    "First read the Twitter and Youtube Comments CSV files (`Twitter-Sentiment.csv` and `YouTubeComments-Sentiment.csv`) and save them in a pandas Dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tw = pd.read_csv('data/Twitter-Sentiment.csv')\n",
    "df_tw.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_yt = pd.read_csv('data/YouTubeComments-Sentiment.csv')\n",
    "df_yt.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2: Run VADER over evaluation texts *(2 points)*\n",
    "\n",
    "#### 2.1 Run VADER over the first tweet\n",
    "\n",
    "In this task you should use VADER for sentiment analysis. For this we use the `vaderSentiment` package. You first have to instantiate a new `SentimentIntensityAnalyzer` and use the `polarity_scores` method of it for the analysis. Apply this for the first tweet. Is it a good classification?\n",
    "\n",
    "[Here](https://github.com/cjhutto/vaderSentiment) under 'Code Examples' you can find some example code how to use this package."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2 Run VADER over each text\n",
    "\n",
    "Now use VADER for all the text data of the Twitter and the Youtube dataframe. Create a new column in the dataframes called `VADER_compound` where you save the `compound` result (look at the output dictonary of the `polarity_scores` method).\n",
    "\n",
    "*Important: Make sure `compound` is a float*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.3 VADER as a classifier\n",
    "\n",
    "To get the three Classes `Positive`, `Negative` and `Neutral` we use the compound score with the following thresholds:\n",
    "\n",
    "* `compound > 0.5`: `\"Positive\"`\n",
    "* `compound < -0.5`: `\"Negative\"`\n",
    "* `else`: `\"Neutral\"`\n",
    "\n",
    "Create a new column called `VADER_class` which contains the three computed classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 3: Use a BERT based model for sentiment analysis *(2 points)*\n",
    "\n",
    "#### 3.1 BERT\n",
    "BERT (Bidirectional Encoder Representation from Transformers) is a machine learning technique for natural language processing. There are already pretrained models available in the `transformers` package. You can look [here](https://huggingface.co/models?sort=downloads&search=sentiment) and choose a model for the next tasks. (We suggest [this](https://huggingface.co/cardiffnlp/twitter-roberta-base-sentiment-latest) (`\"cardiffnlp/twitter-roberta-base-sentiment-latest\"`) model, but you can use any available, just make sure it is suitable for sentiment analysis).\n",
    "\n",
    "First create a `pipeline` where you set your model by the `model` keyword argument. You can then use this method to pass text which should be classified. [Here](https://huggingface.co/blog/sentiment-analysis-python#2-how-to-use-pre-trained-sentiment-analysis-models-with-python) is a tutorial how to use this.\n",
    "\n",
    "As before save the classes in a new column 'BERT_class'. The call to your pipeline returns a dictionary where there is a key `label` which contains already the `positive`, `negative` or `neutral` class (Be aware that this is based on the model you choose, and might be different from the labels in the dataset. If that's the case you have to rename them to match the target labels).\n",
    "\n",
    "***Hint: The classification of the entire sample can take a couple of minutes. Make sure to save the labeled dataset in a csv file so that you don't need to rerun the classification the next time you run your notebook.***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hint -> loading roberta as a pipline\n",
    "sentiment_pipeline = pipeline(model=\"cardiffnlp/twitter-roberta-base-sentiment-latest\", tokenizer=\"cardiffnlp/twitter-roberta-base-sentiment-latest\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hint -> using a pipline for classification\n",
    "sentiment_pipeline('Today is a great day!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 4: Evaluate against sentiment annotations and compare with Google NLP *(4 points)*\n",
    "\n",
    "#### 4.1 Convert GoogleNLP scores to classes\n",
    "\n",
    "As with VADER and BERT, compute classes from the GoogleNLP score, which is given in the column `googleScore`. For this use following thresholds:\n",
    "\n",
    "* `googleScore > 0.3`: `\"Positive\"`\n",
    "* `googleScore < -0.3`: `\"Negative\"`\n",
    "* `else`: `\"Neutral\"`\n",
    "\n",
    "Save the classes in a new column named `GoogleNLP_class`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.2 Evaluate on Twitter\n",
    "\n",
    "First, let's calculate the accuracy for all three classifiers on the Twitter and Youtube data, print the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next calculate the precision of the `\"Positive\"` class for the Twitter and Youtube data.\n",
    "This is calculated as follows:\n",
    "$\n",
    "\\begin{align}\n",
    "    precision = \\frac{TP}{TP + FP}\n",
    "\\end{align}\n",
    "$\n",
    "*Note: Here the Positive samples are the one with the class-label `\"Positive\"`*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now calculate the recall score. This is done by:\n",
    "$\n",
    "\\begin{align}\n",
    "    recall = \\frac{TP}{TP + FN}\n",
    "\\end{align}\n",
    "$\n",
    "*Note: Here the Positive samples are the one with the the class-label `\"Positive\"`*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate the Recall and the Precision score now also for the negative class. \n",
    "\n",
    "The Precision is calculated as:\n",
    "$\n",
    "\\begin{align}\n",
    "    precision = \\frac{TP}{TP + FP}\n",
    "\\end{align}\n",
    "$\n",
    "*Note: Here the Positive samples are the one with the the class-label `\"Negative\"`*\n",
    "\n",
    "And the Recall is calculated as:\n",
    "$\n",
    "\\begin{align}\n",
    "    recall = \\frac{TP}{TP + FN}\n",
    "\\end{align}\n",
    "$\n",
    "*Note: Here the Positive samples are the one with the the class-label `\"Negative\"`*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Last, calculate the [F1 score](https://towardsdatascience.com/the-f1-score-bec2bbc38aa6) of the positive and negative class for each classifier and dataset. The F1 score is calculated as:\n",
    "\n",
    "$\n",
    "\\begin{align}\n",
    "    F_1 = 2 * \\frac{precision * recall}{precision + recall}\n",
    "\\end{align}\n",
    "$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 5: Comparison *(2 points)*\n",
    "* What was the best performing method for Youtube? Did that fit your expectations?\n",
    "* What was the best performing method for Twitter? Did that fit your expectations?\n",
    "* Do you observe any differences between prediction of positive and negative sentiment? What is the role of the imbalance between postive and negative classes in the calculation of accuracy?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2 - Emotion Detection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 6 *(4 points)*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following exercise you will use the emotion classification model [LEIA](https://huggingface.co/LEIA/LEIA-base) to classify the emotion of the sentences in the [enISEAR dataset](https://www.romanklinger.de/data-sets/). You can read more about the `LEIA-base` model in the [documentation](https://huggingface.co/LEIA/LEIA-base) and learn about the implementation details from this [paper](https://arxiv.org/abs/2304.10973)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6.1 LEIA introduction\n",
    "* Load the `LEIA-base` model and tokenize either as a [pipeline](https://huggingface.co/docs/transformers/main_classes/pipelines), or you can load the model and the tokenizer [directly](https://huggingface.co/docs/transformers/autoclass_tutorial) and implement the classification steps by yourself. LEIA only accepts sentences with up to 128 tokens. Make sure that your tokenizer [truncates](https://huggingface.co/docs/transformers/pad_truncation) longer sentences to this lenght to avoid errors.\n",
    "* What are the possible labels the model can predict?\n",
    "* Input the sentence `Today is a great day.` to the model, and predict the emotion of the sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6.2 enISEAR dataset\n",
    "* Load the enISEAR dataset.\n",
    "* What are the possible labels in the dataset? (the `Prior_Emotion` column stores the actual ground-truth label)\n",
    "* The last 7 columns store the number of annotators who chose the given emotion (e.g. if you have the value 3 in the column 'Anger', this means that 3 annotators believed that the sentence in the row expresses Anger). Create a new column `Annotator_Majority_Label`, which stores the emotion with the highest annotator score (i.e. the emotion the highest number of annotators chose for the given sentence).\n",
    "* What percent of the sentences were correctly classified by the (majority vote of the) annotators?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sentence_id</th>\n",
       "      <th>Prior_Emotion</th>\n",
       "      <th>Sentence</th>\n",
       "      <th>Temporal_Distance</th>\n",
       "      <th>Intensity</th>\n",
       "      <th>Duration</th>\n",
       "      <th>Gender</th>\n",
       "      <th>City</th>\n",
       "      <th>Country</th>\n",
       "      <th>Worker_id</th>\n",
       "      <th>Time</th>\n",
       "      <th>Anger</th>\n",
       "      <th>Disgust</th>\n",
       "      <th>Fear</th>\n",
       "      <th>Guilt</th>\n",
       "      <th>Joy</th>\n",
       "      <th>Sadness</th>\n",
       "      <th>Shame</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>271</td>\n",
       "      <td>Fear</td>\n",
       "      <td>I felt ... when my 2 year old broke her leg, a...</td>\n",
       "      <td>Y</td>\n",
       "      <td>Vi</td>\n",
       "      <td>Dom</td>\n",
       "      <td>Ml</td>\n",
       "      <td>Bristol</td>\n",
       "      <td>GBR</td>\n",
       "      <td>87</td>\n",
       "      <td>11/28/2018 00:58:52</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>597</td>\n",
       "      <td>Shame</td>\n",
       "      <td>I felt ... one Christmas as one of our patient...</td>\n",
       "      <td>Y</td>\n",
       "      <td>I</td>\n",
       "      <td>Dom</td>\n",
       "      <td>Fl</td>\n",
       "      <td>Dulwich</td>\n",
       "      <td>GBR</td>\n",
       "      <td>86</td>\n",
       "      <td>11/26/2018 06:52:02</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>282</td>\n",
       "      <td>Guilt</td>\n",
       "      <td>I felt ... because I could not help a friend w...</td>\n",
       "      <td>M</td>\n",
       "      <td>Mi</td>\n",
       "      <td>Dom</td>\n",
       "      <td>Fl</td>\n",
       "      <td>Linlithgow</td>\n",
       "      <td>GBR</td>\n",
       "      <td>83</td>\n",
       "      <td>11/21/2018 18:45:00</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>171</td>\n",
       "      <td>Disgust</td>\n",
       "      <td>I felt ... when I read that hunters had killed...</td>\n",
       "      <td>Y</td>\n",
       "      <td>Mi</td>\n",
       "      <td>H</td>\n",
       "      <td>Ml</td>\n",
       "      <td>Bristol</td>\n",
       "      <td>GBR</td>\n",
       "      <td>87</td>\n",
       "      <td>11/28/2018 00:55:11</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>509</td>\n",
       "      <td>Sadness</td>\n",
       "      <td>I felt ... when my Gran passed away.</td>\n",
       "      <td>Y</td>\n",
       "      <td>Vi</td>\n",
       "      <td>Dom</td>\n",
       "      <td>Fl</td>\n",
       "      <td>Stoke-on-trent</td>\n",
       "      <td>GBR</td>\n",
       "      <td>92</td>\n",
       "      <td>11/26/2018 09:23:38</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Sentence_id Prior_Emotion  \\\n",
       "0          271          Fear   \n",
       "1          597         Shame   \n",
       "2          282         Guilt   \n",
       "3          171       Disgust   \n",
       "4          509       Sadness   \n",
       "\n",
       "                                            Sentence Temporal_Distance  \\\n",
       "0  I felt ... when my 2 year old broke her leg, a...                 Y   \n",
       "1  I felt ... one Christmas as one of our patient...                 Y   \n",
       "2  I felt ... because I could not help a friend w...                 M   \n",
       "3  I felt ... when I read that hunters had killed...                 Y   \n",
       "4               I felt ... when my Gran passed away.                 Y   \n",
       "\n",
       "  Intensity Duration Gender            City Country  Worker_id  \\\n",
       "0        Vi      Dom     Ml         Bristol     GBR         87   \n",
       "1         I      Dom     Fl         Dulwich     GBR         86   \n",
       "2        Mi      Dom     Fl      Linlithgow     GBR         83   \n",
       "3        Mi        H     Ml         Bristol     GBR         87   \n",
       "4        Vi      Dom     Fl  Stoke-on-trent     GBR         92   \n",
       "\n",
       "                  Time  Anger  Disgust  Fear  Guilt  Joy  Sadness  Shame  \n",
       "0  11/28/2018 00:58:52      0        0     0      1    0        3      1  \n",
       "1  11/26/2018 06:52:02      1        0     0      4    0        0      0  \n",
       "2  11/21/2018 18:45:00      0        0     0      4    0        1      0  \n",
       "3  11/28/2018 00:55:11      3        0     0      0    0        2      0  \n",
       "4  11/26/2018 09:23:38      0        0     0      0    0        5      0  "
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_isear = pd.read_csv('data/enISEAR.tsv', sep='\\t')\n",
    "df_isear.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6.3 Classification\n",
    "* Drop the rows from the enISEAR dataset, where the `Prior_Emotion` is not one of `Fear`, `Sadness`, `Anger` or `Joy`\n",
    "* Use `Leia` to classify the emotion of each remaining sentence in the dataset, and add a column `Leia_Label` to store the predicted classes\n",
    "* Now remove `I felt ... ` (or variations of it) from the beginning of each sentence, and rerun the classfication. Store your results in a column named `Leia_Label_Clean`\n",
    "* Where the model predicted `Happiness` or `Affection`, change the prediction to `Joy` to match the dataset's labels (for both columns -> `Leia_Label` and `Leia_Label_Clean`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6.4 Analysis\n",
    "* Compare the performance of the two approaches, with each other, as well as with the performance of the human majority using the metrics introduced in part 1 (accuracy, precision, recall, f1 score) or other metrics you find interesting. Create informative visualizations to aid the comparison.\n",
    "* Discuss your results. \n",
    "* Are the models accurately predicting human emotions?\n",
    "* Which approach seems to work better? Why?\n",
    "* What kind of other/additional preprocessing could we perform to improve the model's predictions?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 7 *(6 points)*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 7.1 Data annotation\n",
    "* In the following exercise you will need to test emotion detection methods on data from [Vent](https://www.vent.co/), a website where users talk about their feelings. \n",
    "* On GitHub, in the `a03/data` folder you can find 2 files. First open `data_for_labeling.csv`. This file contains 100 texts. Import the data and sample 30 random texts. Remember to set a seed and save the sample to a separate csv. This is the sample, for which you are supposed to label the emotion in each sentence. Feel free to do this in the csv (i.e. in Excel). The possible classes are: 0 (Sadness), 1 (Affection), 2 (Fear), 3 (Happiness), 4 (Anger). ***Important: Make sure to upload the labeled data with your submission.***\n",
    "* After you finished labeling the data load it as a pandas dataframe. Also load `data_with_labels.csv` as a dataframe, which contains the actual labels of the 100 rows of data.\n",
    "* Merge the two dataframes (so that you end up with the 30 rows, you sampled. Including your labels and the ground truth), and rename the column containing your labels as `label_human`.\n",
    "* Rename the class ids (0, 1, 2, ...) stored in the `label`, and `label_human` columns to the class names (Sadness, Affection, ...)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 7.2 LEIA\n",
    "* Use the [LEIA](https://huggingface.co/LEIA/LEIA-base) model introduced in the previous exercise to classify the sentences and store the results in a column named `label_leia`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 7.3 Open Models from the Hugging Face Model Hub\n",
    "\n",
    "* In the following exercise we will work with the free [Huggin Face Inference API](https://huggingface.co/docs/api-inference/en/index), an API which allows you to access many, also very powerful open AI models. Your task will be to use a [text completion](https://huggingface.co/docs/api-inference/en/quicktour) model to classify the sentences, you and LEIA labeled above (your sample of 30 rows), according to their emotion (Sadness, Affection, Fear, Happiness, Anger).\n",
    "\n",
    "* You have to sign up for the API by creating an account on the [Hugging Face page](https://huggingface.co/join). The free version is rate limited, which should not pose a problem with the number of examples in this exercise.\n",
    "\n",
    "* After you login, create an API key on this page in the settings: https://huggingface.co/settings/tokens\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"src/hf_key.png\" alt=\"HF token\" style=\"width: 600px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* We recommend to use [Meta-Llama-3-8B-Instruct](https://huggingface.co/meta-llama/Meta-Llama-3-8B-Instruct) for this task. If you want, you can also use another, powerful recent model, but you will have to adapt the prompting format and maybe other parameters to be able to do that.\n",
    "\n",
    "* Run the model on the texts and extract the label from the model's response (Hint: `.split(\"<|start_header_id|>assistant<|end_header_id|>\")` can help you with that).\n",
    "\n",
    "* Store the results in a column named `label_llama3`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import pandas as pd\n",
    "import random\n",
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>I'm so busy this weekend that I have no time t...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>I don't even know what I'm doing for my birthday.</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  label\n",
       "0  I'm so busy this weekend that I have no time t...      4\n",
       "1  I don't even know what I'm doing for my birthday.      2"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Just an example with the whole (100 row) data set here\n",
    "df = pd.read_csv(\"data/data_with_labels.csv\")\n",
    "\n",
    "df = df.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "sents = df['text'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Great now my head hurts too'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# take a look at one example text\n",
    "sents[25]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# enter your key here\n",
    "API_TOKEN=\"\"\n",
    "\n",
    "API_URL = \"https://api-inference.huggingface.co/models/meta-llama/Meta-Llama-3-8B-Instruct\"\n",
    "headers = {\"Authorization\": f\"Bearer {API_TOKEN}\"}\n",
    "\n",
    "\n",
    "# few-shot prompting llama3\n",
    "def query(text):\n",
    "    \n",
    "    payload={\n",
    "    \"inputs\": \"<|begin_of_text|>\\\n",
    "    <|start_header_id|>system<|end_header_id|>\\\n",
    "    You are now Emotbot.\\\n",
    "    Emotbot answers with only one of the following words: Sadness, Affection, Fear, Happiness, Anger.\\\n",
    "    If Emotbot answers with anything else they have failed. Emotbot cannot fail.\\\n",
    "    Emotbot will be provided with short pieces of text from the website 'Vent'.\\\n",
    "    'Vent' is a website where users can share their feelings to an anonymous audience.\\\n",
    "    When writing the 'Vent' text, the authors selected 1 of the following emotions to represent what they were feeling: 'Sadness', 'Affection', 'Fear', 'Happiness', or 'Anger'.\\\n",
    "    Emotbot will read and analyse the text and predict which of the 5 feelings the author had selected.\\\n",
    "    <|eot_id|>\\\n",
    "    \\\n",
    "    <|start_header_id|>user<|end_header_id|>I hate fuckin every single person on this fuckin planet. Someone kill me pls<|eot_id|>\\\n",
    "    <|start_header_id|>assistant<|end_header_id|>Sadness<|eot_id|>\\\n",
    "    \\\n",
    "    <|start_header_id|>user<|end_header_id|>Best day I have had in a long time :)<|eot_id|>\\\n",
    "    <|start_header_id|>assistant<|end_header_id|>Happiness<|eot_id|>\\\n",
    "    \\\n",
    "    <|start_header_id|>user<|end_header_id|>boy I like called me princess Heâ€™s so precious<|eot_id|>\\\n",
    "    <|start_header_id|>assistant<|end_header_id|>Affection<|eot_id|>\\\n",
    "    \\\n",
    "    <|start_header_id|>user<|end_header_id|>\"+text+\"<|eot_id|><|start_header_id|>assistant<|end_header_id|>\"\n",
    "    }\n",
    "    \n",
    "    response = requests.post(API_URL, headers=headers, json=payload)\n",
    "    return response.json()\n",
    "\n",
    "\n",
    "output = query(sents[25])\n",
    "\n",
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the label\n",
    "\n",
    "output[0]['generated_text'].split(\"<|start_header_id|>assistant<|end_header_id|>\")[4]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 7.4 Comparison\n",
    "* Compare the performance of the two models, with each other, as well as with the quality of your annotation using the metrics introduced in part 1 (accuracy, precision, recall, f1 score) or other metrics you find interesting. Create informative visualizations to aid the comparison.\n",
    "* Discuss your results. \n",
    "* Are the models accurately predicting human emotions?\n",
    "* Which approach seems to work better? Why?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "46e2835a142a16ae115bce5fddf19f27ce13b17a4ab8ded638c88ab5ce5171d2"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
